# -*- coding: utf-8 -*-
"""Image_Classification_Rock_Paper_Scissor_UpdateWith_Callbacks

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WlhZFeF9imq6yAyK3NHstg9_icTAgn5n

# **Proyek Akhir : Klasifikasi Gambar**



*   Nama : Ahmad Zakki Fadhilurrahman
*   Email : ahmaedjk3@gmail.com
*   id Dicoding : ahmadz18017

# **Import Library and Packages**
"""

# Commented out IPython magic to ensure Python compatibility.
#import seluruh library
import tensorflow as tf
import numpy as np
import pandas as pd

import os
import cv2
import imghdr
import fnmatch

from google.colab import files
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

"""# **Import dan Ekstrak Dataset Yang Diperlukan**

**File Dapat Diunduh Pada : [Tautan Ini](https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip)**
"""

#Import dataset rockpaperscissor,zip yang merupakan kumpulan gambar rockpaperscissors sebagai training dan validation image
!wget --no-check-certificate \
  https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip \
  -O /content//rockpaperscissors.zip

"""**Proses Ekstraksi Dilakukan dengan Menggunakan Packages zipfile, os**"""

#Ekstraksi pada file yang diperlukan
import zipfile, os

file_zip='/content/rockpaperscissors.zip' #Lokasi Ekstraksi
zip_ref= zipfile.ZipFile(file_zip, 'r')
zip_ref.extractall('/content') #Tujuan ekstraksi
zip_ref.close() #Menutup zipfile

"""# **Eksplorasi Dan Persiapan Data Train Validasi**"""

#Mengecek list folder directori pada folder rockpaperscissors

os.listdir('/content/rockpaperscissors')
#terdapat 5 file dengan file yang digunakan yaitu paper, scissors dan rock, rps-cv-images (untuk basis directori)
#Apabila split volder sudah pernah di run sebelumnya makan akan ada tambahan 2 folder yaitu train dan val

#Menginstal PIP tambahan yaitu split_folders untuk memanagement pembagian dataset dan foldernya
!pip install split_folders

import splitfolders #Import library splitfolder

basis_direktori='/content/rockpaperscissors/rps-cv-images' #memilih basis direktori yaitu rps-cv-images dimana dalam folder ini terdapat 3 folder saja yang dibutuhkan yaitu rock, paper, dan scissor
splitfolders.ratio(basis_direktori, output='/content/rockpaperscissors', seed= 1337, ratio=(0.6, 0.4)) #rasio pembagian train dan validation disesuaikan dengan perintah yaitu 6/4

train_direktori= os.path.join('/content/rockpaperscissors', 'train') #mengidentifikasikan folder train yang berisi data train yang diperlukan (yaitu di hasil splitfolder)
validation_direktori= os.path.join('/content/rockpaperscissors', 'val') #mengidentifikasikan folder val yang berisi data validation

"""**Fungsi Menghitung Total File train dan val**"""

train_dir='/content/rockpaperscissors/train'
val_dir='/content/rockpaperscissors/val'

#Membuat Fungsi Untuk Menghitung dan Menentukan Informasi pada Direktori yang Telah Diekstraksi

list_of_tuples= []

def information_data(dir_data) :
  for image_class in os.listdir(dir_data):
    for image in os.listdir(os.path.join(dir_data, image_class)) :
      image_path = os.path.join(dir_data, image_class, image)
      img = cv2.imread(image_path)
      list_of_tuples.append(img.shape)


size_of_dir =[]

def counting_dir(directory) :
  count = len(fnmatch.filter(os.listdir(directory), '*.*'))
  size_of_dir.append(count)

train_rock= '/content/rockpaperscissors/train/rock'
train_paper= '/content/rockpaperscissors/train/paper'
train_scissor= '/content/rockpaperscissors/train/scissors'
val_rock= '/content/rockpaperscissors/val/rock'
val_paper= '/content/rockpaperscissors/val/paper'
val_scissor= '/content/rockpaperscissors/val/scissors'

counting_dir(train_rock)
counting_dir(train_paper)
counting_dir(train_scissor)
counting_dir(val_rock)
counting_dir(val_paper)
counting_dir(val_scissor)

information_data(train_dir)
information_data(val_dir)

#Converting to dataframe for info
df = pd.DataFrame(list_of_tuples, columns=['tinggi_pixel', 'lebar_pixel', 'RGB'])
df.describe()

size_of_dir

df2 = pd.DataFrame(size_of_dir, columns=['count'],index=['Train_Rock', 'Train_Paper', 'Train_Scissors', 'Val_Rock', 'Val_Paper','Val_Scissors'])
df2

"""**VIsualisasi Jumlah Data yang Dimiliki**"""

plt.figure()
plt.bar(df2.index, df2['count'], color = ['black','orange','silver', 'black','orange','silver'], alpha =.8)
plt.xticks(rotation =30)
plt.title('Jumlah Label Untuk Setiap Training dan Validation Data')
plt.show()

"""Dari Informasi Diatas kita dapat melihat bahwa seluruh data memiliki ukuran yang sama untuk tinggi dan lebar pixel serta memiliki warna RGB. Selain itu, sebaran jumlah data yang dimiliki juga menunjukkan jumlah yang tidak berbeda jauh sehingga data dapat langsung digunakan tanpa harus dikurangi

# **Melakukan proses Augmentasi Gambar dan Persiapan data train & validation**

Proses Augmentasi digunakan untuk melakukan penambahan variasi data
"""

#Augmentasi digunakan untuk data train dengan ImageDataGenerator
train_datagen= ImageDataGenerator(
    rescale=1./255, #Rescalse gambar
    horizontal_flip=True, #menggenerasi gambar dengan variasi horizontal flip di setiap gambarnya
    rotation_range=20, #Menggenerasi variasi gambar rotasi 20 derajat
    shear_range=0.2, #Menggenerasi gambar dengan variasi pergeseran
    zoom_range=[0.8,1.2], #Menggenerasi gambar dengan variasi zoom 0.8-1.2
    brightness_range=[0.75,2.0], #mengenerasi gambar dengan variasi terang 0.75-2.0
    fill_mode='nearest' #defalut nearest
)
test_datagen= ImageDataGenerator(
    rescale=1./255, #validation image disesuaikan rescalenya dengan data train
)

train_generator =train_datagen.flow_from_directory(
    train_direktori, #Direktori data latih
    target_size=(150,150), #mengubah resolusi seluruh gambar menjadi 150x150
    batch_size=32, #Batch yang saya gunakan 32
    #karena ini merupakan masalah klasifikasi lebih dari 2 sehingga digunakan categorical
    class_mode='categorical',
    shuffle=True
    )

validation_generator =test_datagen.flow_from_directory(
    validation_direktori, #Direktori data validasi
    target_size=(150, 150), #Mengubah resolusi seluruh gambar menjadi 150x150 piksel
    batch_size=32,
    #karena ini merupakan masalah klasifikasi lebih dari 2 sehingga digunakan categorical
    class_mode='categorical',
    shuffle= False
    )

"""# **Melakukan Pembuatan Model**

Menggunakan beberapa layer Convolusi 2D dan Maxpooling satu layer Flatten dan 2 layer Dense Menggunakan activation function relu (Rectified Linear Unit) dan Softmax
"""

model= tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(512, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

#Model menggunakan input 150x150 dengan tiga katogory
#Untuk tiga kelas atau lebih menggunkan fungsi aktivasi softmax

model.summary() #Summary Model

"""**Menambahkan Callbacks**

Callbacks untuk menghentikan proses training apabila target accuracy dan loss telah terpenuhi untuk proses train dan validation
"""

#Callbacks

class Callback(tf.keras.callbacks.Callback) :
  def on_epoch_end(self, epoch, logs={}) :
    if (logs.get('accuracy') > 0.97 and logs.get('loss') < 0.10 and logs.get('val_loss') < 0.10 and logs.get('val_accuracy')>0.97) :
      print(f"\nSelamat Model Telah Berhasil Dilatih Dengan Akurasi Train {logs.get('accuracy')} dan Akurasi Validasi {logs.get('val_accuracy')} Tercapai !!!\ndengan loss train {logs.get('loss')} dan loss validasi {logs.get('val_loss')}")
      self.model.stop_training = True
callbacks = Callback()

"""**Mengcompile Model**"""

model.compile(loss='categorical_crossentropy',
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])
#Menggunakan optimizer Adam dengan loss function yaitu sparse_categorical_crossentropy untuk tiga kelas

"""# **Melakukan Pelatihan pada Model**"""

#Latih model dengan pengatoran model.fit sebagai berikut
graph_model = model.fit(
    train_generator,
    steps_per_epoch=25,
    epochs=20,
    validation_data=validation_generator,
    validation_steps=5,
    verbose=2,
    callbacks=[callbacks]
)

"""# **Melakukan Plot Hasil dan Menguji Model**"""

#Tambahan untuk melihat plot hasil pelatihan (train loss dan validation loss) tiap epochnya
def show_graph(history):
    fig, ax = plt.subplots(1, 2, figsize=(18,5))
    ax[0].set_title('loss')
    ax[0].plot(history.epoch, history.history["loss"], label="Train loss")
    ax[0].plot(history.epoch, history.history["val_loss"], label="Validation loss")
    ax[1].set_title('accuracy')
    ax[1].plot(history.epoch, history.history["accuracy"], label="Train acc")
    ax[1].plot(history.epoch, history.history["val_accuracy"], label="Validation acc")
    ax[0].legend()
    ax[1].legend()

show_graph(graph_model)
model_score= model.evaluate(validation_generator, steps=20)
print("Model Test Loss:",model_score[0])
print("Model Test Accuracy:",model_score[1])

#Pengaturan upload
uploaded=files.upload()

for fn in uploaded.keys():
  #Prediccting images
  path =fn
  img= image.load_img(path, target_size=(150,150))

  imgplot=plt.imshow(img)
  x= image.img_to_array(img)
  x= np.expand_dims(x, axis=0)
  images= np.vstack(([x]))

  classes= model.predict(images, batch_size=10)
  print(fn)
  if classes [0][0]==1 :
    print('paper')
  elif classes [0][1]==1:
    print('rock')
  else:
    print('scisors')

print(classes)
#Pengaturan output dimmana [0][0]==1 merupakan paper (folder pertama) [0][1]==1 rock (folder kedua), dan [1][1] scissor (folder ketiga)
#Pengaturan ini dilakukan berdasarkan urutan direktori yang digunakan

"""# Unused Code

rock_direktori= os.path.join(basis_direktori, 'rock')
paper_direktori= os.path.join(basis_direktori, 'paper')
scissor_direktori= os.path.join(basis_direktori, 'scissors')

train_rock= os.path.join(train_direktori, 'rock')
train_paper= os.path.join(train_direktori, 'paper')
train_scissor= os.path.join(train_direktori, 'scissors')
val_rock= os.path.join(validation_direktori,'rock')
val_paper= os.path.join(validation_direktori,'paper')
val_scissor= os.path.join(validation_direktori,'scissors')
"""

#meembagi direktori menjadi train dan validasi
'''
train_rock, val_rock= train_test_split(os.listdir(rock_direktori), train_size=0.6)
train_paper, val_paper= train_test_split(os.listdir(paper_direktori), train_size=0.6)
train_scissor, val_scissor= train_test_split(os.listdir(scissor_direktori), train_size=0.6)
'''

'''
train_datagen= ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    horizontal_flip=True,
    vertical_flip=True,
    shear_range=0.2,
    brightness_range=[0.2,1.7],
    zoom_range=[0.4, 1.0],
    fill_mode='nearest'
)
test_datagen= ImageDataGenerator(
    #rescale=1./255,
    rotation_range=20,
    #horizontal_flip=True,
    #vertical_flip=True,
    shear_range=0.2,
    brightness_range=[0.5,2.0],
    #zoom_range=[0.4, 1.0],
    fill_mode='nearest'
)



train_generator =train_datagen.flow_from_directory(
    train_direktori, #Direktori data latih
    target_size=(150,150), #mengubah resolusi seluruh gambar menjadi 150x150
    batch_size=32,
    # karena ini merupakan masalah klasifikasi 2 kelas, gunakan class_mode = 'binary'
    class_mode='categorical')

validation_generator =test_datagen.flow_from_directory(
    validation_direktori, #Direktori data validasi
    target_size=(150, 150), #Mengubah resolusi seluruh gambar menjadi 150x150 piksel
    batch_size=32,#karena ini merupakan masalah klasifikasi 2 kelas gunakan binary
    class_mode='categorical')



'''

'''
#Code untuk mencoba 1 gambar seperti apa image generator dihasilkan
from numpy import expand_dims
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import ImageDataGenerator
from matplotlib import pyplot
sample=expand_dims(img_to_array(load_img('/tmp/rockpaperscissors/scissors/0CSaM2vL2cWX6Cay.png')), 0)

it= test_datagen.flow(sample, batch_size=1)
for i in range(9) :
  pyplot.subplot(330+1+i)
  batch=it.next()
  image=batch[0].astype('uint8')
  pyplot.imshow(image)
pyplot.show()
'''